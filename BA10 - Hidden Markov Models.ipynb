{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 10: Hidden Markov Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro: finding CpG-islands \n",
    "\n",
    "CpG dinucleotides are the least common due to cytosine methylation increasing the rate of deamination (C->T).\n",
    "So, dinucleotide CG (reverse complement is also CG, btw) is depleted in many genomes. However, methylated is suppresed around genes in 'CpG islands'. We can think of CpG island (or not) as a hidden state in a HMM.\n",
    "\n",
    "\n",
    "#### HMM probabilities:\n",
    "\n",
    "Since coin flips are independent events, the probability that $n$ flips of the fair coin will generate a given sequence $x = x_1, x_2, ..., x_n $ with $k$ occurrences of “$Heads$” is\n",
    "\n",
    "$ \\mathrm{Pr}(x|\\textit{F}) = \\displaystyle\\prod_{i=1}^{n}\\mathrm{Pr}_F(x_i) = \\left(1/2\\right)^n $\n",
    "\n",
    "On the other hand, the probability that a ($p(H)=1/4$) biased coin will generate the same sequence is \n",
    "\n",
    "$ \\mathrm{Pr}(x|B) =\\displaystyle\\prod_{i=1}^{n}\\mathrm{Pr}_B(x_i) = \\left(1/4\\right)^{n-k} \\cdot \\left(3/4\\right)^k = 3^k/4^n $\n",
    "\n",
    "-  **If $Pr(x|F) > Pr(x|B)$**, then the dealer more likely used a fair coin, \n",
    "-  else if $Pr(x|F) < Pr(x|B)$, then the dealer more likely used a biased coin. \n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "The numbers $(1/2)^n$ and $3^k/4^n$ are so small for large n that in order to compare them, we will use their **log-odds ratio**:  \n",
    "\n",
    "<br>  \n",
    "\n",
    "$ \\log_2{\\left(\\dfrac{\\mathrm{Pr}(x|F)}{\\mathrm{Pr}(x|B)}\\right)} = \\log_2{\\left(\\dfrac{2^n}{3^k}\\right)} = n - k \\cdot \\log_2{3} $\n",
    "\n",
    "<br>\n",
    " \n",
    "**Exercise Break**: Show that Pr(x|F) is larger than Pr(x|B) when the log-odds ratio is positive (i.e., when k/n < 1/ log2(3)) and smaller than Pr(x|B) when the log-odds ratio is negative (i.e., when k/n > 1/ log2(3)).\n",
    "<br>\n",
    "\n",
    "**note** big $\\displaystyle\\prod_{i=1}^{n}$ means the product of probabilites $\\mathrm{Pr}_{State}(x_i)$.\n",
    "Recall, multiple independent events in sequence (1 and 2 and...) -> use *'the product rule'*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Each **emitted string $x$ has probability $Pr(x)$, which is *independent of the hidden path* taken by the HMM**:\n",
    "\n",
    "$ \\mathrm{Pr}(x) = \\displaystyle\\sum_{\\text{all hidden paths } \\pi} \\mathrm{Pr}(x, \\pi)$\n",
    "\n",
    "Each hidden path π has probability Pr(π), which is independent of the string that the HMM emits:\n",
    "\n",
    "$\\mathrm{Pr}(\\pi) = \\displaystyle\\sum_{\\text{all~strings~of~emitted~symbols } x}\\mathrm{Pr}(x, \\pi)$\n",
    "\n",
    "The event “the HMM follows the hidden path π and emits x” can be thought of as a combination of two consecutive events:\n",
    "\n",
    "- The HMM follows the path π. The probability of this event is Pr(π).\n",
    "- The HMM emits x, given that the HMM follows the path π. \n",
    "  We refer to the probability of this event as the conditional probability of x given π, denoted Pr(x|π).\n",
    "- Both of these events must occur for the HMM to follow path π and emit string x, which implies that\n",
    "\n",
    "$ Pr(x, π) = Pr(x|π) · Pr(π) $\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BA10A: Compute the Probability of a Hidden Path \n",
    "\n",
    "---\n",
    "\n",
    "**Probability of a Hidden Path Problem**  \n",
    "\n",
    "*Given*: \n",
    "    A hidden path $π$ followed by the states States and transition matrix *Transition* of an HMM (Σ, States, Transition, Emission).\n",
    "\n",
    "*Return*: \n",
    "    The probability of this path, $Pr(π)$. You may assume that initial probabilities are equal.  \n",
    "\n",
    "Probability of a Hidden Path Problem: Compute the probability of a hidden path.\n",
    "\n",
    "**Input**: A hidden path π in an HMM (Σ, States, Transition, Emission).\n",
    "**Output**: The probability of this path, Pr(π).\n",
    "\n",
    "Sample Input:\n",
    "\n",
    "    ABABBBAAAA\n",
    "    --------\n",
    "    A B\n",
    "    --------\n",
    "            A\tB\n",
    "        A\t0.377\t0.623\n",
    "        B\t0.26\t0.74\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def p_hiddenpath(hidden_path, T_matrix, states):\n",
    "    \"\"\"Calculates the probability of a hiddenpath given a T_matrix HMM\"\"\"\n",
    "    # parse hidden path to integers corresponding to E, T matrices\n",
    "    hidden_path = [int(states.index(state)) for state in hidden_path]\n",
    "\n",
    "    # assume transition from the initial state occur with equal probability.\n",
    "    probability = 0.5\n",
    "    prev = hidden_path[0]\n",
    "    for state in hidden_path[1:]:\n",
    "#         print(state, '->', prev)\n",
    "#         print('trans_p = ', trans_probability)\n",
    "#         print('prob = ;', probability)\n",
    "        trans_probability = T_matrix[prev][state]\n",
    "        probability = probability * trans_probability\n",
    "        prev = state\n",
    "    \n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.377 0.623]\n",
      " [0.26  0.74 ]] ABABBBAAAA ['A', 'B']\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/jasonmoggridge/Dropbox/Rosalind/Coursera_textbook_track/Course6/data/10a_test.txt\") as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "    hidden_path = lines[0].strip()\n",
    "    states = lines[2].split()\n",
    "    T_matrix = np.array([line.split()[1:] for line in lines[5:]], float)\n",
    "    \n",
    "print(T_matrix, hidden_path, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pr(hidden_path) = 0.0003849286917546758\n"
     ]
    }
   ],
   "source": [
    "print(\"Pr(hidden_path) =\", p_hiddenpath(hidden_path, T_matrix, states))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct answer is: \n",
    "0.0003849286917546758\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.955720786186205e-16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/jasonmoggridge/Dropbox/Rosalind/Coursera_textbook_track/Course6/data/dataset_26255_8.txt\") as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "    hidden_path = lines[0].strip()\n",
    "    states = lines[2].split()\n",
    "    T_matrix = np.array([line.split()[1:] for line in lines[5:]], float)\n",
    "    \n",
    "p_hiddenpath(hidden_path, T_matrix, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Good news for you, correct! (answer for test data: 7.955720786186205e-16)\n",
    "<br>\n",
    "\n",
    " ---\n",
    " \n",
    "Note that we have already computed $Pr(x|π)$ for the **crooked dealer HMM** when the dealer’s hidden path consisted only of B or F (fair or biased coin), which we wrote as $Pr(x|B)$ and $Pr(x|F)$, respectively (the probabilities of heads given biased or fair coins).   \n",
    "\n",
    "To compute $Pr(x|π)$ for a general HMM, we will write $Pr(x_i|π_i)$ to denote the emission probability $emission_{π_i}(x_i) $ that symbol $x_i$ was emitted given that the HMM was in state $π_i$.  \n",
    "\n",
    "As a result, for a given path $π$, the HMM emits a string $x$ with probability equal to the product of emission probabilities along that path,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### BA10B: probability of emission seq given hidden path and emission matrix\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Input**: A string x, followed by the alphabet from which x was constructed, followed by a hidden path π, followed by the states States and emission matrix Emission of an HMM (Σ, States, Transition, Emission).\n",
    "\n",
    "<br> \n",
    "\n",
    "**Output**: The conditional probability Pr(x|π) that x will be emitted given that the HMM follows the hidden path π.\n",
    "\n",
    "<br>\n",
    "\n",
    "Sample Input:\n",
    "\n",
    "    zzzyxyyzzx                    # emission string x\n",
    "    --------\n",
    "    x y z                         # emission alphabet\n",
    "    --------\n",
    "    BAAAAAAAAA                    # hiddenpath pi\n",
    "    -------- \n",
    "    A B                           # hidden States\n",
    "    --------\n",
    "        x\ty\tz\n",
    "    A\t0.176\t0.596\t0.228\n",
    "    B\t0.225\t0.572\t0.203     # Emission matrix\n",
    "\n",
    "\n",
    "Sample Output:\n",
    "\n",
    "    3.59748954746e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_emissions(emissions, alphabet, hidden_path, states, e_matrix):\n",
    "    \"\"\" calculate probability of this emission string given that hidden_path and Ematrix \"\"\"\n",
    "    hidden_path = [int(states.index(state)) for state in hidden_path]\n",
    "    emissions = [int(alphabet.index(em)) for em in emissions]\n",
    "    probability = 1\n",
    "    for i in range(len(emissions)):\n",
    "        probability = probability * e_matrix[hidden_path[i]][emissions[i]]\n",
    "    print(probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: zzzyxyyzzx alphabet ['x', 'y', 'z'] \n",
      " hidden_path BAAAAAAAAA states ['A', 'B'] \n",
      "\n",
      " e_matrix \n",
      " [[0.176 0.596 0.228]\n",
      " [0.225 0.572 0.203]] \n",
      "\n",
      "\n",
      "Solution:\n",
      "3.5974895474624624e-06\n"
     ]
    }
   ],
   "source": [
    "with open(\"/Users/jasonmoggridge/Dropbox/Rosalind/Coursera_textbook_track/Course6/data/10b_test.txt\") as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "    emissions = lines[0].strip()\n",
    "    alphabet = lines[2].strip().split()\n",
    "    hidden_path = lines[4].strip()\n",
    "    states = lines[6].strip().split()\n",
    "    e_matrix = np.array([line.split()[1:] for line in lines[9:]], float)\n",
    "    \n",
    "print('string:', emissions,'alphabet', alphabet,'\\n', 'hidden_path', hidden_path,'states',states,'\\n\\n', 'e_matrix','\\n',e_matrix, '\\n')\n",
    "print('\\nSolution:')\n",
    "p_emissions(emissions, alphabet, hidden_path, states, e_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution:\n",
      "1.1523621771156757e-26\n"
     ]
    }
   ],
   "source": [
    "# test data\n",
    "\n",
    "with open(\"/Users/jasonmoggridge/Dropbox/Rosalind/Coursera_textbook_track/Course6/data/dataset_26255_10.txt\") as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "    emissions = lines[0].strip()\n",
    "    alphabet = lines[2].strip().split()\n",
    "    hidden_path = lines[4].strip()\n",
    "    states = lines[6].strip().split()\n",
    "    e_matrix = np.array([line.split()[1:] for line in lines[9:]], float)\n",
    "    \n",
    "# print('string:', emissions,'\\n', 'alphabet', alphabet,'\\n\\n\\n', 'hidden_path', hidden_path,'\\n\\n',\n",
    "#       'states',states,'\\n\\n', 'e_matrix','\\n',e_matrix, '\\n\\n\\n')\n",
    "print('\\nSolution:')\n",
    "p_emissions(emissions, alphabet, hidden_path, states, e_matrix)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Solution:\n",
    "1.1523621771156757e-26\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### BA10C: Viterbi Algorithm for max-likelihood hidden path\n",
    "\n",
    "\n",
    "Returning to our formula for Pr(x, π):  \n",
    "the **probability that an HMM follows path π and emits string x** can be written as a product of emission and transition probabilities:\n",
    "\n",
    "$ \\begin{aligned} \\mathrm{Pr}(x, \\pi) & = \\color{blue}{\\mathrm{Pr}(x|\\pi)} \\cdot \\color{green}{\\mathrm{Pr}(\\pi)}\\\\ & = \\displaystyle\\prod_{i=1}^{n} {\\,\\color{blue}{\\mathrm{Pr}(x_i|\\pi_i)} \\cdot \\color{green}{\\mathrm{Pr}(\\pi_{i-1} \\rightarrow \\pi_i)}}\\\\ & = \\displaystyle\\prod_{i=1}^{n}{\\,\\color{purple}{\\textit{emission}_{\\pi_i}(x_i)} \\cdot \\color{crimson}{\\textit{transition}_{\\pi_{i-1},\\,\\pi_i}}}\\,. \\end{aligned} $\n",
    " \n",
    "\n",
    "Exercise Break: Compute Pr(x, π) for the x and π in the reproduced figure below. Can you find a better explanation for x = “THTHHHTHTTH” than π = FFFBBBBBFFF?\n",
    "\n",
    "STOP and Think: Now that you have learned about HMMs, try designing an HMM that will model searching for CG-islands in a genome. What barriers do you encounter? \n",
    "-  how to define what a CG-island is: what is the emisson prob of CGisland vs not?\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "#### HMM -> The decoding problem and Viterbi algorithm\n",
    "\n",
    "\n",
    "As we stated in the previous section, in both the crooked dealer and CG-island HMMs, we are **looking for the most likely hidden path π for an HMM that emits a string x**. In other words, we would like to **maximize Pr(x, π) among all possible hidden paths π**.  (an optimization problem) <br>  \n",
    "\n",
    "##### Decoding Problem: \n",
    "\n",
    "-  Find an optimal hidden path in an HMM given a string of its emitted symbols.\n",
    "\n",
    "*Input*: A string x = x1 . . . xn emitted by an HMM (Σ, States, Transition, Emission). <br>\n",
    "*Output*: A path π that maximizes the probability Pr(x, π) over all possible paths through this HMM. <br>\n",
    "\n",
    "---\n",
    "\n",
    "In 1967, Andrew Viterbi used an **HMM-inspired analog of a Manhattan-like grid to solve the Decoding Problem**. \n",
    "For an HMM emitting a string of n symbols $x = x_1 . . . x_n$, the nodes in the HMM’s Viterbi graph are divided into $|States|$ rows and $n$ columns.\n",
    "\n",
    "##### Viterbi graph: \n",
    "-  $node(k, i)$ represents state $k$ and the $i$-th emitted symbol. \n",
    "-  Each node is connected to all nodes in the column to its right; \n",
    "-  the edge connecting $(l, i − 1)$ to $(k, i)$ corresponds to transitioning from state $l$ to state $k$ \n",
    "   - (with probability $transition(l,k)$ \n",
    "-  and then emitting symbol $x_i$ (with probability $emission_k(x_i)$). \n",
    "-  As a result, every path connecting a node in the first column of the Viterbi graph to a node in the final column corresponds to a hidden path $π = π_1 . . . π_n$.\n",
    "\n",
    "We assign a weight of: $\\textit{Weight}_{i}(l,k) = \\color{green}{\\,\\textit{transition}_{\\pi_{i-1}, \\pi_{i}}} \\cdot \\color{purple}{\\textit{emission}_{\\pi_i} (x_i)}$\n",
    "\n",
    "to the edge connecting $(l, i − 1)$ to $(k, i)$ in the Viterbi graph. Furthermore, we define the **product weight of a path in the Viterbi graph as the product of its edge weights**. For a path from the leftmost column to the rightmost column in the Viterbi graph corresponding to the hidden path π, this product weight is equal to the product of n − 1 terms,\n",
    "\n",
    "$\n",
    "\\displaystyle\\prod_{i=2}^{n} {\\color{green}{\\,\\textit{transition}_{\\pi_{i-1},\\,\\pi_i}}} \\cdot {\\color{purple}{\\textit{emission}_{\\pi_i} (x_i)}} = \\displaystyle\\prod_{i=2}^{n}{\\,\\textit{Weight}_{i}(l,k)} \n",
    "$\n",
    "\n",
    "\n",
    "￼STOP and Think: How does this expression differ from the formula for Pr(x, π) that we derived in the previous section?\n",
    "\n",
    "<img src=http://bioinformaticsalgorithms.com/images/HMM/HMM_diagram_three_states.png width=\"120\">\n",
    "<img src=http://bioinformaticsalgorithms.com/images/HMM/Viterbi_graph_three_states.png width=\"500\">\n",
    "\n",
    "**Figure**: (Top) The diagram of an HMM with three states (emission/transition probabilities as well as nodes corresponding to emitted symbols are omitted). (Bottom) Given a string of n symbols x = x1 . . . xn emitted by an HMM, Viterbi’s Manhattan is a grid with |States| rows and n columns in which each node is connected to every node in the column to its right. The weight of the edge connecting (l, i − 1) to (k, i) is Weight(l, k) = transitionl, k · emissionk(xi). Unlike the alignment graphs we encountered previously, in which the set of valid directions was restricted to south, east, and southeast edges, every node in a column is connected by an edge to every node in the column to its right in the Viterbi graph.\n",
    "\n",
    "The only difference between the expression:  $\\displaystyle\\prod_{i=2}^{n} {\\color{green}{\\,\\textit{transition}_{\\pi_{i-1},\\,\\pi_i}}} \\cdot {\\color{purple}{\\textit{emission}_{\\pi_i} (x_i)}} = \\displaystyle\\prod_{i=1}^{n-1}{\\,\\textit{Weight}_{i}(l,k)} \n",
    "$\n",
    "\n",
    "and the expression that we obtained for $Pr(x, π)$:   $\\displaystyle\\prod_{i=1}^{n} \\,{\\color{green}{\\textit{transition}_{\\pi_{i-1},\\,\\pi_i}}} \\cdot {\\color{purple}{\\textit{emission}_{\\pi_i} (x_i)}} \n",
    "$\n",
    "\n",
    "is the single factor $transition _{π_0, π_1} · emission_{π_1}(x1),$ which corresponds to transitioning from the initial state $π_0$ to $π_1$ and emitting the first symbol. To model the initial state, we will add a source node source to the Viterbi graph and then connect source to each node (k, 1) in the first column with an edge of weight <br>\n",
    "$Weight_1(source, k) = transition_{π_0, k} · emission_k(x_1) $. <br>\n",
    "\n",
    "We will also assume that the HMM has another silent **terminal state** that the HMM enters when it has finished emitting symbols. To model the terminal state, we add a sink node sink to the Viterbi graph and connect every node in the last column to sink with an edge of weight 1 (see figure below).\n",
    "\n",
    "<img src=http://bioinformaticsalgorithms.com/images/HMM/Viterbi_graph_three_states_source_sink.png width=\"440\">\n",
    "\n",
    "**Figure**: The Viterbi graph with additional source node (blue) and sink node (red). A path of largest product weight connecting the source to the sink corresponds to an optimal hidden path solving the Decoding Problem.\n",
    "\n",
    "#### Seems like just a simple dynamic programming problem, as with Needleman-Wunsch alignment (global).\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every hidden path π in the HMM now corresponds to a path from source to sink in the Viterbi graph with product weight Pr(x, π). Therefore, the Decoding Problem reduces to finding a path in the Viterbi graph of largest product weight over all paths connecting source to sink.\n",
    "\n",
    "￼Exercise Break: Find the maximum product weight path in the Viterbi graph for the crooked dealer HMM (whose HMM diagram is reproduced below) when x = “HHTT”.  Express your answer as a string of four \"F\" and \"B\" symbols.\n",
    "\n",
    "Note: You may assume that transitions from the initial state occur with equal probability.\n",
    "\n",
    "<img src=http://bioinformaticsalgorithms.com/images/HMM/HMM_diagram_complete.png width=\"400\">\n",
    "\n",
    "\n",
    "\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.25 b1: 0.375\n",
      "\n",
      "f2: 0.1125 B? False\n",
      "b2 0.25312500000000004 B? True\n",
      "\n",
      "f3 0.050625 from B? False\n",
      "b3: 0.056953125000000014 B? True\n",
      "\n",
      "F4 0.022781250000000003 B? False\n",
      "b4 0.012814453125000003 B? True\n"
     ]
    }
   ],
   "source": [
    "F= 0.5\n",
    "B= {'heads':0.75, 'tails':0.25}\n",
    "t = [0.9, 0.1]\n",
    "\n",
    "heads ='heads'\n",
    "ff = (0.5)*F\n",
    "bb = 0.5*B[heads]\n",
    "print('f1:', ff,'b1:', bb)\n",
    "\n",
    "#2\n",
    "b = bb*t[1]*B[heads]\n",
    "f = ff*t[0]*F\n",
    "new_f = max(b, f)\n",
    "print('\\nf2:', new_f, 'B?',b>f)\n",
    "\n",
    "b = bb*t[0]*B[heads]\n",
    "f = ff*t[1]*F\n",
    "new_b = max(b, f)\n",
    "print('b2', new_b, 'B?',b>f)\n",
    "\n",
    "\n",
    "bb = new_b; ff = new_f\n",
    "\n",
    "tails = 'tails'\n",
    "#F3 #B3\n",
    "b = bb*t[1]*B[tails]\n",
    "f = ff*t[0]*F\n",
    "new_f = max(b, f)\n",
    "print('\\nf3', new_f, 'from B?', b>f)\n",
    "b = bb*t[0]*B[tails]\n",
    "f = ff*t[1]*F\n",
    "new_b = max(b, f)\n",
    "print('b3:', new_b, 'B?',b>f)\n",
    "bb = new_b;ff = new_f\n",
    "\n",
    "#4\n",
    "b = bb*t[1]*B[tails]\n",
    "f = ff*t[0]*F\n",
    "new_f = max(b, f)\n",
    "print('\\nF4',new_f,'B?',b>f)\n",
    "\n",
    "b = bb*t[0]*B[tails]\n",
    "f = ff*t[1]*F\n",
    "new_b = max(b, f)\n",
    "print('b4',new_b,'B?',b>f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the hidden path that maximizes the probability of that outcome (HHTT), given the T and E matrices is 'FFFF'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "#### **The Viterbi algorithm**\n",
    "\n",
    "<br>\n",
    "\n",
    "We will apply a dynamic programming algorithm to solve the Decoding Problem. \n",
    "\n",
    "First, define $s_{k,i}$ as the product weight of an optimal path (i.e., a path with maximum product weight) from source to the node $(k, i)$. \n",
    "\n",
    "The Viterbi algorithm relies on the fact that the first $i − 1$ edges of an optimal path from source to $(k, i)$ must form an optimal path from source to $(l, i − 1)$ for some (unknown) state $l$. This observation yields the following recurrence (perfect substructure):\n",
    "\n",
    "$\n",
    "\\begin{aligned} s_{k,\\,i} & = \\displaystyle\\max_{\\text{all states }l} \\left\\{ s_{l,\\,i-1} \\cdot (\\text{weight of edge between nodes}(l, i-1)\\text{ and }(k, i)) \\right\\}\\\\ & = \\displaystyle\\max_{\\text{all states } l} \\left\\{ s_{l,\\,i-1} \\cdot \\textit{Weight}_i(l,k) \\right\\}\\\\ & = \\displaystyle\\max_{\\text{all states } l} \\left\\{ s_{l,\\,i-1} \\cdot \\color{green}{\\textit{transition}_{\\pi_{i-1},\\,\\pi_i}} \\cdot {\\color{purple}{\\textit{emission}_{\\pi_i} (x_i)}} \\right\\} \\end{aligned} $\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Since source is connected to every node in the first column of the Viterbi graph,\n",
    "\n",
    "$\\begin{aligned} s_{k,\\,1} & = s_{\\textit{source}} \\cdot \\left(\\text{weight of edge between } \\textit{source} \\text{ and } (k, 1)\\right)\\\\ & = s_{\\textit{source}} \\cdot \\textit{Weight}_0(\\textit{source}, k)\\\\ & = s_{\\textit{source}} \\cdot {\\color{green}{\\textit{transition}_{\\textit{source},\\,k}}} \\cdot {\\color{purple}{\\textit{emission}_k(x_1)}} \\end{aligned} $\n",
    "\n",
    " \n",
    "\n",
    "In order to initialize this recurrence, we set $source$ equal to 1. We can now compute the maximum product weight over all paths from $source$ to $sink$ as\n",
    "\n",
    "$ s_{\\textit{sink}} = \\displaystyle\\max_{\\text{all states } l}s_{l,\\,n} $\n",
    "\n",
    "￼STOP and Think: How can we adapt our algorithm for finding a longest path in a DAG to find a path with maximum product weight?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How fast is the Viterbi algorithm?\n",
    "We can interpret the Decoding Problem as yet another instance of the Longest Path in a DAG Problem from our work on sequence alignment because the path π maximizing the product weight \n",
    "\n",
    "$ \\prod_{i=1}^{n} \\textit{Weight}_i(\\pi_{i-1}, \\pi_i) $\n",
    "\n",
    "also maximizes the **logarithm of this product**, which is equal to:\n",
    "\n",
    "$ \\sum_{i=1}^{n} \\log{(\\textit{Weight}_i(\\pi_{i-1}))} $\n",
    "\n",
    "\n",
    "Thus, we can substitute the weights of all edges in the Viterbi graph by their logarithms. \n",
    "Finding a longest path (i.e. a path maximizing the **sum** of edge weights) in the resulting graph will correspond to a path of maximum product weight in the original Viterbi graph. \n",
    "For this reason, the runtime of the Viterbi algorithm, which you are now ready to implement, is **linear in the number of edges in the Viterbi graph**. \n",
    "The following exercise shows that the number of these edges is $O(|States|^2·n)$, where n is the number of emitted symbols.\n",
    "\n",
    "**Exercise Break**: Show that the number of edges in the Viterbi graph of an HMM emitting a string of length $n$ is $|States|^2·(n−1)+2·|States|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The viterbi graph has $|states|*n$ nodes: to connect each of |states| nodes to each of |states| nodes the next column requires $|states|^2$ edges and we must do this $(n-1)$ times to create the grid portion of the graph. This accounts for $|states|^2*(n-1)$ edges.\n",
    "\n",
    "The grid must be connected to source and sink nodes, thus requiring $2*|states|$ edges.\n",
    "\n",
    "The sum of these is  $|states|^2*(n-1) + 2*|states| $ edges, \n",
    "\n",
    "presumably |states| << n, so we ignore the 2*|states| portion for bigO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "Code Challenge: Implement the Viterbi algorithm solving the Decoding Problem.\n",
    "\n",
    "Input: A string x, followed by the alphabet from which x was constructed, followed by the states States, transition matrix Transition, and emission matrix Emission of an HMM (Σ, States, Transition, Emission).\n",
    "\n",
    "Output: A path that maximizes the (unconditional) probability Pr(x, π) over all possible paths π.\n",
    "\n",
    "Note: You may assume that transitions from the initial state occur with equal probability.\n",
    "\n",
    "Sample Input:\n",
    "\n",
    "        xyxzzxyxyy\n",
    "        --------\n",
    "        x y z\n",
    "        --------\n",
    "        A B\n",
    "        --------\n",
    "            A\tB\n",
    "        A\t0.641\t0.359\n",
    "        B\t0.729\t0.271\n",
    "        --------\n",
    "            x\ty\tz\n",
    "        A\t0.117\t0.691\t0.192\t\n",
    "        B\t0.097\t0.42\t0.483\n",
    "\n",
    "Sample Output:\n",
    "\n",
    "    AAABBAAAAA\n",
    "\n",
    "---\n",
    "\n",
    "<br> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VITERBI ALGORITHM: BIOINFO ALGOS- ba10c\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def parse_HMM(lines):\n",
    "    emission = lines[0].strip()\n",
    "    alphabet = lines[2].strip().split()\n",
    "    emission = [int(alphabet.index(em)) for em in emission]\n",
    "    states = lines[4].strip().split()\n",
    "    S = len(states)\n",
    "    T = np.array([line.split()[1:] for line in lines[7:7+S]], float)\n",
    "    T = np.log(T)\n",
    "    E = np.array([line.split()[1:] for line in lines[9+S:]], float)\n",
    "    E = np.log(E)\n",
    "    return(emission, alphabet, states, T, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_hiddenpath(emission, T, E, states, alphabet):\n",
    "    \"\"\"returns max likelihood hidden path for emission string, given HMM\"\"\"\n",
    "    S = len(states)\n",
    "    n = len(emission)\n",
    "    viterbi = np.ones(shape = (S, n)) * -float('inf')\n",
    "    pointers = [[False for e in range(n)] for s in range(S)] \n",
    "    # init first column of viterbi with Pr_emission & 1/States\n",
    "    for state in range(S):\n",
    "        viterbi[state][0] = np.log(1/S) + E[state][emission[0]]\n",
    "        pointers[state][0] = -1\n",
    "    # Fill viterbi graph using dynamic programming\n",
    "    for i in range(1,n):\n",
    "        for state in range(S):\n",
    "            for prev in range(S):\n",
    "                p_total = E[state][emission[i]] + T[prev][state] + viterbi[prev][i-1]\n",
    "                if p_total > viterbi[state][i]:\n",
    "                    viterbi[state][i] = p_total\n",
    "                    pointers[state][i] = prev\n",
    "    # start backtrack from greatest probability in last column of viterbi\n",
    "    score = -float('inf')\n",
    "    for state in range(S):\n",
    "        if viterbi[state][n-1] > score:\n",
    "            last = state\n",
    "            score = viterbi[state][n-1]\n",
    "    path = [last]\n",
    "    # backtrack to recreate max likelihood hidden_path in reverse\n",
    "    i = n-1\n",
    "    while i > 0:\n",
    "        next = pointers[last][i]\n",
    "        path.append(next)\n",
    "        last = next\n",
    "        i -= 1\n",
    "    # reverse string to get hidden_path     \n",
    "    return ''.join(str(states[state]) for state in path[::-1])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: [0, 1, 0, 2, 2, 0, 1, 0, 1, 1] \n",
      "alphabet ['x', 'y', 'z'] states ['A', 'B'] \n",
      "\n",
      "E_matrix\n",
      " [[-2.14558134 -0.36961546 -1.65025991]\n",
      " [-2.3330443  -0.86750057 -0.72773863]] \n",
      "\n",
      "T_matrix\n",
      " [[-0.44472582 -1.02443289]\n",
      " [-0.31608155 -1.30563646]]\n",
      "\n",
      "solved -> AAABBAAAAA\n",
      "\tsolution: AAABBAAAAA\n"
     ]
    }
   ],
   "source": [
    "## Sample data1:\n",
    "with open(\"/Users/jasonmoggridge/Dropbox/Rosalind/Coursera_textbook_track/Course6/data/10c_test.txt\") as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "emission, alphabet, states, T, E = parse_HMM(lines)\n",
    "print('string:', emission,'\\nalphabet', alphabet,'states',states,'\\n\\nE_matrix\\n',E, '\\n\\nT_matrix\\n', T)\n",
    "print('\\nsolved ->',viterbi_hiddenpath(emission, T, E, states, alphabet))\n",
    "print(\"\\tsolution: AAABBAAAAA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: [2, 0, 0, 0, 0, 1, 2, 2, 0, 1, 0, 1, 0, 1, 2, 0, 2, 2, 0, 2, 2, 2, 1, 2, 2, 0, 0, 0, 2, 0, 0, 1, 1, 1, 2, 0, 1, 0, 2, 1, 0, 1, 0, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 2, 2, 0, 2, 0, 2, 1, 2, 2, 2, 2, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 2, 1, 1, 2, 1, 1, 1, 0, 2, 2, 2, 2, 1, 2, 0, 1, 2, 2, 1, 1, 1] \n",
      "alphabet ['x', 'y', 'z'] states ['A', 'B'] \n",
      "\n",
      "E_matrix\n",
      " [[-0.63111179 -1.48722028 -1.42295835]\n",
      " [-0.78307189 -1.65025991 -1.04696906]] \n",
      "\n",
      "T_matrix\n",
      " [[-0.45570632 -1.00512195]\n",
      " [-0.94933059 -0.48939034]]\n",
      "\n",
      "solved   -> AAAAAAAAAAAAAABBBBBBBBBBBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABBBBBBBBBBBAAAAAAAAAAAAAAAAAAAAABBBBBBBBBBAAA\n",
      "solution -> AAAAAAAAAAAAAABBBBBBBBBBBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABBBBBBBBBBBAAAAAAAAAAAAAAAAAAAAABBBBBBBBBBAAA\n",
      "== ? True\n"
     ]
    }
   ],
   "source": [
    "## extra sample data:\n",
    "\n",
    "ans = 'AAAAAAAAAAAAAABBBBBBBBBBBAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABBBBBBBBBBBAAAAAAAAAAAAAAAAAAAAABBBBBBBBBBAAA'\n",
    "\n",
    "with open(\"/Users/jasonmoggridge/Dropbox/Rosalind/Coursera_textbook_track/Course6/data/10c_test2.txt\") as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "    emission, alphabet, states, T, E = parse_HMM(lines)\n",
    "    print('string:', emission,'\\nalphabet', alphabet,'states',states,'\\n\\nE_matrix\\n',E, '\\n\\nT_matrix\\n', T)\n",
    "\n",
    "print('\\nsolved   ->',viterbi_hiddenpath(emission, T, E, states, alphabet))\n",
    "print(\"solution ->\", ans)\n",
    "print(\"== ?\", viterbi_hiddenpath(emission, T, E, states, alphabet) == ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: [1, 0, 0, 2, 1, 0, 0, 2, 1, 2, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 2, 1, 1, 0, 2, 1, 0, 1, 0, 1, 1, 0, 1, 1, 2, 0, 1, 0, 0, 0, 0, 2, 0, 1, 0, 0, 2, 1, 1, 0, 0, 2, 0, 2, 2, 2, 1, 0, 1, 0, 1, 1, 2, 1, 0, 0, 0, 2, 2, 1, 1, 0, 0, 2, 0, 2, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 2, 1, 1, 0, 2, 2, 0, 0, 1, 1, 0, 2, 0, 0] \n",
      "alphabet ['x', 'y', 'z'] states ['A', 'B', 'C', 'D'] \n",
      "\n",
      "E_matrix\n",
      " [[-1.09661429 -1.53711725 -0.79628794]\n",
      " [-2.24431618 -0.83471074 -0.77652879]\n",
      " [-0.94933059 -1.5945493  -0.89159812]\n",
      " [-0.44005655 -2.18925641 -1.41058705]] \n",
      "\n",
      "T_matrix\n",
      " [[-0.95451194 -1.33560125 -1.47403328 -2.09557092]\n",
      " [-1.77195684 -0.9063404  -1.37436579 -1.75446368]\n",
      " [-0.80073239 -1.05268336 -2.02495336 -2.65926004]\n",
      " [-2.3330443  -1.70374859 -1.5847453  -0.66164851]]\n",
      "\n",
      "solved   -> BDDDDDDDDDDDBBDDBBBBBBBCBBCBCBBCBBBDDDDDDDDDDDBBBDDDDBBBBCBCBBBBDDDBBBBDDDDDDDBBBCBBCBBBBDDDDDBBDDDD\n",
      "solution -> BDDDDDDDDDDDBBDDBBBBBBBCBBCBCBBCBBBDDDDDDDDDDDBBBDDDDBBBBCBCBBBBDDDBBBBDDDDDDDBBBCBBCBBBBDDDDDBBDDDD\n",
      "== ? True\n"
     ]
    }
   ],
   "source": [
    "## Actual test data:\n",
    "\n",
    "with open(\"/Users/jasonmoggridge/Dropbox/Rosalind/Coursera_textbook_track/Course6/data/dataset_26256_7.txt\") as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "    emission, alphabet, states, T, E = parse_HMM(lines)\n",
    "    print('string:', emission,'\\nalphabet', alphabet,'states',states,'\\n\\nE_matrix\\n',E, '\\n\\nT_matrix\\n', T)\n",
    "\n",
    "    \n",
    "ans='BDDDDDDDDDDDBBDDBBBBBBBCBBCBCBBCBBBDDDDDDDDDDDBBBDDDDBBBBCBCBBBBDDDBBBBDDDDDDDBBBCBBCBBBBDDDDDBBDDDD'\n",
    "print('\\nsolved   ->',viterbi_hiddenpath(emission, T, E, states, alphabet))\n",
    "print(\"solution ->\", ans)\n",
    "print(\"== ?\", viterbi_hiddenpath(emission, T, E, states, alphabet) == ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "**Exercise Break**: Apply your solution for the **Decoding Problem to find CG-islands in the first million nucleotides from the human X chromosome** (given in FASTA format). To help you design an HMM for this application, you may assume that transitions from CG-islands to non-CG-islands are rare, occurring with probability 0.001, and that transitions from non-CG-islands to CG-islands are even more rare, occurring with probability 0.0001. How many CG-islands do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "T = np.array([[1-10**-4, 10**-4],[10**-3, 1-10**-3]])\n",
    "\n",
    "# data from 'finding CG islands' slides\n",
    "freq0 = np.array([53,79,127,36,37,58,58,41,35,75,81,26,24,105,115,50])/1000\n",
    "freq1 = np.array([87,58,84,61,67,63,17,63,53,53,63,42,51,70,84,84])/1000\n",
    "Emission_matrix = np.array([freq0,freq1])\n",
    "\n",
    "np.save(\"T_matrix\", T)\n",
    "np.save(\"E_matrix\", Emission_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.053, 0.079, 0.127, 0.036, 0.037, 0.058, 0.058, 0.041, 0.035,\n",
       "         0.075, 0.081, 0.026, 0.024, 0.105, 0.115, 0.05 ],\n",
       "        [0.087, 0.058, 0.084, 0.061, 0.067, 0.063, 0.017, 0.063, 0.053,\n",
       "         0.053, 0.063, 0.042, 0.051, 0.07 , 0.084, 0.084]]),\n",
       " array([[9.999e-01, 1.000e-04],\n",
       "        [1.000e-03, 9.990e-01]]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E = np.load(\"E_matrix.npy\")\n",
    "T = np.load(\"T_matrix.npy\")\n",
    "E, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "States = [\"non-island\", \"CG-island\"]\n",
    "nt = 'ACGT'\n",
    "alpha = dict(zip([a+b for a in nt for b in nt],range(16)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "with open(\"/Users/jasonmoggridge/Dropbox/Rosalind/Coursera_textbook_track/Course6/data/chrX.txt\", 'r') as file:\n",
    "    xchromosome = SeqIO.read(file, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SeqRecord(seq=Seq('NNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNNN...NNN', SingleLetterAlphabet()), id='chrX', name='chrX', description='chrX', dbxrefs=[])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xchromosome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**finished the viterbi algorithm**  \n",
    "time for 10d. Let's go.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic programming allows us to answer questions about HMMs extending beyond the most likely hidden path. For example, we have already computed the probability Pr(π) of a hidden path π. But what about computing Pr(x), the probability that the HMM will emit a string x?\n",
    "\n",
    "STOP and Think: Which outcome is more likely in the crooked casino: “HHTT” or “HTHT”? How would you find the most likely sequence of four coin flips?\n",
    "\n",
    "#### Outcome Likelihood Problem: Find the probability that an HMM emits a given string.\n",
    "\n",
    "    - Input: A string x = x1 . . . xn emitted by an HMM (Σ, States, Transition, Emission).\n",
    "\n",
    "    - Output: The probability Pr(x) that the HMM emits x.\n",
    "    \n",
    "    \n",
    "    STOP and Think: To solve the Outcome Likelihood Problem, you can make a slight change to the Viterbi recurrence,\n",
    "\n",
    "\\begin{aligned} s_{k,\\,i} = \\max_{\\text{all states } l} \\{s_{l,\\,i-1} \\cdot \\textit{Weight}_i(l, k)\\}s \\end{aligned}\n",
    "\n",
    "What is the change?\n",
    "\n",
    "We have already observed that Pr(x) is equal to the sum of Pr(x, π) over all hidden paths π. However, the number of paths through the Viterbi graph is exponential in the length of the emitted string x, and so we will use dynamic programming to develop a faster approach to compute Pr(x).\n",
    "\n",
    "We denote the total product weight of all paths from source to node (k, i) in the Viterbi graph as forwardk, i; note that forwardsink is equal to Pr(x). To compute forwardk,i, we will divide all paths connecting source to (k, i) into |States| subsets, where each subset contains those paths that pass through node (l, i − 1) (with product weight forwardl, i−1) before reaching (k, i) for some l between 1 and |States|. Therefore, forwardk, i is the sum of |States| terms,\n",
    "\n",
    "\\begin{aligned} \\textit{forward}_{k,\\,i} & = \\displaystyle\\sum_{\\text{all states } l }{\\textit{forward}_{l,\\,i-1} \\cdot \\left({\\text{weight of edge connecting } (l, i-1)\\text{ and }(k, i)}\\right)}\\\\ & = \\displaystyle\\sum_{\\text{all states } l } {\\textit{forward}_{l,\\, i-1} \\cdot \\textit{Weight}_i(l,k)}\\,. \\end{aligned} \n",
    "\n",
    "\n",
    "\n",
    "Note that the only difference between this recurrence and the Viterbi recurrence,\n",
    "\n",
    "\\begin{aligned} s_{k,\\,i} = \\displaystyle\\max_{\\text{all states } l} \\left\\{ s_{l,\\,i-1} \\cdot \\textit{Weight}_i(l,k) \\right\\} \\end{aligned}\n",
    "\n",
    "is that the maximization in the Viterbi algorithm has changed into a summation symbol. \n",
    "We can now **solve the Outcome Likelihood Problem by computing** $forward_{sink}$, which is equal to:\n",
    "\n",
    "\\begin{aligned} \\sum_{\\text{all states } k}\\textit{forward}_{k,\\,n} \\end{aligned}\n",
    "\n",
    "\n",
    "### BA10D: the Outcome Likelihood Problem.\n",
    "---\n",
    "\n",
    "**Input**:  A string x, followed by the alphabet from which x was constructed, followed by the states States, transition matrix Transition, and emission matrix Emission of an HMM (Σ, States, Transition, Emission).\n",
    "\n",
    "\n",
    "**Output**: The probability Pr(x) that the HMM emits x.\n",
    "\n",
    "**Note**: You may assume that transitions from the initial state occur with equal probability.\n",
    "\n",
    "Sample Input:\n",
    "\n",
    "    xzyyzzyzyy\n",
    "    --------\n",
    "    x y z\n",
    "    --------\n",
    "    A B\n",
    "    --------\n",
    "        A\tB\n",
    "    A\t0.303\t0.697 \n",
    "    B\t0.831\t0.169 \n",
    "    --------\n",
    "        x\ty\tz\n",
    "    A\t0.533\t0.065\t0.402 \n",
    "    B\t0.342\t0.334\t0.324\n",
    "\n",
    "Sample Output:\n",
    "    1.1005510319694847e-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VITERBI ALGORITHM: BIOINFO ALGOS- ba10d\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def parse_HMM(lines):\n",
    "    emission = lines[0].strip()\n",
    "    alphabet = lines[2].strip().split()\n",
    "    emission = [int(alphabet.index(em)) for em in emission]\n",
    "    states = lines[4].strip().split()\n",
    "    S = len(states)\n",
    "    T = np.array([line.split()[1:] for line in lines[7:7+S]], float)\n",
    "    E = np.array([line.split()[1:] for line in lines[9+S:]], float)\n",
    "    return(emission, states, T, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def outcome_likelihood(emission, T, E, states): # outcome-likelihood of HMM emitting emissions (sum of all hidden paths)\n",
    "    \"\"\"returns likelihood of emission string, given HMM\"\"\"\n",
    "\n",
    "    S = len(states)\n",
    "    n = len(emission)\n",
    "    viterbi = np.zeros(shape = (S, n))\n",
    "\n",
    "    # init first column of viterbi with Pr_emission & 1/States\n",
    "    for state in range(S):\n",
    "        viterbi[state][0] = 1/S * E[state][emission[0]]\n",
    "\n",
    "    # Fill viterbi graph with sums over all incoming edges for ea node\n",
    "    for i in range(1,n):\n",
    "        for state in range(S):\n",
    "            em = E[state][emission[i]]\n",
    "            for prev in range(S):\n",
    "                trans = T[prev][state]\n",
    "                viterbi[state][i] += trans * em * viterbi[prev][i-1]\n",
    "    return sum(viterbi[s][n-1] for s in range(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: [0, 2, 1, 1, 2, 2, 1, 2, 1, 1] \n",
      "states ['A', 'B'] \n",
      "\n",
      "E_matrix\n",
      " [[0.533 0.065 0.402]\n",
      " [0.342 0.334 0.324]] \n",
      "\n",
      "T_matrix\n",
      " [[0.303 0.697]\n",
      " [0.831 0.169]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.100551031969485e-06"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/jasonmoggridge/Dropbox/Rosalind/Coursera_textbook_track/Course6/data/10d_test.txt\") as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "    emission, states, T, E = parse_HMM(lines)\n",
    "    print('string:', emission,'\\nstates',states,'\\n\\nE_matrix\\n',E, '\\n\\nT_matrix\\n', T)\n",
    "\n",
    "\n",
    "outcome_likelihood(emission, T, E, states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string: [2, 1, 2, 2, 2, 0, 2, 1, 0, 1, 2, 2, 1, 2, 1, 0, 1, 1, 2, 2, 0, 2, 2, 2, 2, 1, 2, 0, 1, 1, 1, 2, 1, 1, 1, 0, 2, 1, 2, 1, 1, 0, 1, 2, 2, 2, 1, 1, 2, 0, 1, 2, 1, 0, 1, 2, 1, 1, 2, 1, 2, 2, 0, 2, 0, 0, 2, 0, 2, 1, 0, 2, 0, 0, 2, 2, 2, 0, 2, 2, 2, 0, 1, 0, 1, 2, 2, 2, 0, 0, 1, 2, 0, 2, 2, 2, 1, 1, 0, 1] \n",
      "states ['A', 'B', 'C'] \n",
      "\n",
      "E_matrix\n",
      " [[0.377 0.378 0.245]\n",
      " [0.395 0.009 0.596]\n",
      " [0.194 0.183 0.623]] \n",
      "\n",
      "T_matrix\n",
      " [[0.361 0.342 0.297]\n",
      " [0.194 0.394 0.412]\n",
      " [0.293 0.365 0.342]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.105412913344748e-50"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"/Users/jasonmoggridge/Dropbox/Rosalind/Coursera_textbook_track/Course6/data/dataset_26257_4.txt\") as f:\n",
    "    lines = [line.strip() for line in f]\n",
    "    emission, states, T, E = parse_HMM(lines)\n",
    "    print('string:', emission,'\\nstates',states,'\\n\\nE_matrix\\n',E, '\\n\\nT_matrix\\n', T)\n",
    "\n",
    "\n",
    "outcome_likelihood(emission, T, E, states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct ans: 3.105412913344748e-50\n",
    "\n",
    "done all of BA10 A-D\n",
    "\n",
    "<br>\n",
    "\n",
    "#### Most Likely Outcome Problem:\n",
    "---\n",
    "\n",
    "Now that we can compute Pr(x) for an emitted string x, a natural question is to find the most likely such string. In the crooked dealer example, this corresponds to finding the most likely sequence of flips over all possible sequences of fair and biased coins that the dealer could use.\n",
    "\n",
    "\n",
    "Find a most likely string emitted by an HMM.\n",
    "\n",
    "**Input**: An HMM (Σ, States, Transition, Emission) and an integer n.\n",
    "\n",
    "**Output**: A most likely string x = x1 . . . xn emitted by this HMM, i.e., a string maximizing the probability Pr(x) that the HMM will emit x.\n",
    "\n",
    "**Exercise Break:** Solve the Most Likely Outcome Problem (Hint: You may need to build a 3-dimensional version of Viterbi’s Manhattan).\n",
    "\n",
    "##### My solution:\n",
    "---\n",
    "Yes. it would need an extra dimension with a 2d viterbi manhattan for each symbol in the emission alphabet. So we would have a $|states|*|emission seq|*|alphabet|$ 3d grid, and edges from all (i-i, state, symbol) nodes in all graphs to each (i,state,symbol) nodes.\n",
    "\n",
    "We'd fill out the viterbi grid in the same was as earlier, storing the max weight path to each node in the grid, then walk back along the path, getting the optimal hidden path and their emissions.\n",
    "\n",
    "---\n",
    "\n",
    "<br>  <br>  <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
